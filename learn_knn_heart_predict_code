import pandas as pd
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.compose import make_column_transformer
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV

# Wczytanie danych
df = pd.read_csv('heart_attack_prediction_india.csv')

# Usunięcie niepotrzebnych kolumn ID, Health_Insurance
df = df.drop(columns= ['Patient_ID','Emergency_Response_Time','Health_Insurance'])

# Podział na cechy (X) i etykiety (y)
X = df.drop(columns=['Heart_Attack_Risk'])
y = df['Heart_Attack_Risk']

# One-Hot Encoding i przekształcenie kolumn kategorycznych
ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False).set_output(transform='pandas')
ct = make_column_transformer(
    (ohe, ['State_Name', 'Gender']),
    remainder = 'passthrough'
)
# Podział na zbiór treningowy i testowy
X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=10, test_size= 0.2)

# Transformacja danych kategorycznych
X_train = ct.fit_transform(X_train)
X_test = ct.transform(X_test)

# Skalowanie danych
scaler = MinMaxScaler(feature_range=(0,1))
X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)

# Definiujemy zakres hiperparametrów do przetestowania
param_grid = {
    'n_neighbors': [3,5,10,15,20],   # Testujemy różne liczby sąsiadów
    'weights': ['uniform', 'distance'],  # Równomierne ważenie vs. ważenie odległością
    'metric': ['euclidean', 'manhattan', 'chebyshev']  # Różne metryki odległości
}


# Tworzenie i trenowanie modelu KNN
knn = KNeighborsClassifier()

# Tworzymy obiekt GridSearchCV
grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=1)

# Przeprowadzamy przeszukiwanie
grid_search.fit(X_train, y_train)

# Wypisujemy najlepsze hiperparametry
print("Najlepsze parametry:", grid_search.best_params_)

# Wypisujemy najlepszy wynik F1-score
print("Najlepszy wynik F1-score:", grid_search.best_score_)
knn.fit(X_train, y_train)

# Używamy najlepszego modelu do predykcji
best_knn = grid_search.best_estimator_
y_pred = best_knn.predict(X_test)

# Ocena modelu
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Jeśli model ma dużo FN (fałszywie negatywnych przypadków), może to oznaczać, że nie wykrywa
# wystarczająco dobrze osób zagrożonych atakiem serca
# 1343 - brak choroby TN
# 82 przypadki oznaczone jako brak ryzyka, ale choroa wystąpiła FN
# 547 błędnie oznaczone jako „ryzyko” (fałszywe alarmy) FP
# 28 poprawnie przewidziane przypadki „ryzyka”. TP


cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['No Risk', 'Risk'], yticklabels=['No Risk', 'Risk'])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

#próbuje zmieniać po kolei liczbę najbliższych sąsiadów
